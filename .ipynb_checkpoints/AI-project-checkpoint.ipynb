{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcde863e-2626-4768-9b16-d9f5696e0d08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Projet IA : Vérification de la véracité des informations concernant COVID19. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e68613-46d2-469c-9fc4-afa1bff2eeef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importation des dépendances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a6f2e-d226-4cf3-a559-1f32a7b2904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importation des librairies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Visualisation\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "from tqdm import tqdm, tqdm_notebook # show progress bar\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Natural Language Toolkit\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a9568-bb0e-4239-a72f-b46a71a17ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation du dataset\n",
    "df = pd.read_excel(\"Data-FakeRealCOVID.xlsx\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58bba2-7b81-4631-9933-7d13ca8a06fa",
   "metadata": {},
   "source": [
    "## Exploration du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c909b2e-7d11-4071-9acb-e5895b0a708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3387c-b37b-4371-8c11-30e656b6b5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b9993-ce84-4674-a20a-2093f7497f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lenght : \",len(df))\n",
    "print(\"size :\",df.size)\n",
    "print(\"shape : \",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9cfbe-3ba4-46d9-ac75-3515877296bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose  = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e1f8e-00b2-4f70-9a4d-9177120c21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea567bfc-3bbb-4033-80af-d72b21ea3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vérification des données manquantes\n",
    "df.columns[df.isnull().any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9533a-017d-416f-86d8-03b3161a6348",
   "metadata": {},
   "source": [
    "On a pas de données manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379000a-3962-4ce8-9570-2fd8d1f97e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voir les valeurs possible de la collone label\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eebfe7-0cd3-4e5b-8e7e-53c713d53f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ddddc-8fbe-442f-807d-2821765d4387",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbaf5e0-bf02-498b-8f3d-f490c9237744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation \n",
    "\n",
    "label = df.groupby('label').count()\n",
    "idx = label.index.tolist()\n",
    "values= label['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1035ba-40d2-46f3-9579-d2a5686cedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(x_range=idx, title=\"Distribution of data\",\n",
    "           toolbar_location=None, tools=\"\")\n",
    "\n",
    "p.vbar(x=idx, top=values, width=0.9)\n",
    "\n",
    "p.xgrid.grid_line_color = None\n",
    "\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa160f4-db05-4520-aacd-726b21fe3646",
   "metadata": {},
   "source": [
    "## Prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1602c03c-64b1-4f4a-9023-97d394649e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df[['tweet','label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe4992-d084-4e49-a21b-2f530a81595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On remplace les labels par des données binaire\n",
    "data['label'] = data['label'].replace('real',1)\n",
    "data['label'] = data['label'].replace('fake',0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b51c70-7ba4-49f0-821a-ef590e17dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a730810-a999-4add-975a-d4341b38291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on crée un dataset tel que les données sont bien séparés\n",
    "data_real = data[data['label'] == 1]\n",
    "data_fake = data[data['label'] == 0]\n",
    "dataset = pd.concat([data_real, data_fake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d07ae0-9a24-40b1-a6d4-a58c976401fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tweetsave']=dataset['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad95fab-aab9-40db-abb5-6e51932cac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions pour nettoyage des données\n",
    "\n",
    "## supprimer les emojis \n",
    "def deEmojify(text):\n",
    "    return text.encode(\"ascii\", \"ignore\").decode()\n",
    "## séparer les hashtags en des mots \n",
    "def clean_hash(text):\n",
    "    s = \"\"\n",
    "    for word in str(text).split():\n",
    "        if word.startswith(\"#\"):\n",
    "            word=  \" \".join([a for a in re.split('([A-Z][a-z]+)', word) if a])\n",
    "        s+= word+' '\n",
    "    return s\n",
    "## supprimer les mentions \n",
    "def remove_mentions(text):\n",
    "    return re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "## supprimer les urls \n",
    "def clean_url(text):\n",
    "    return re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text)\n",
    "## supprimer la ponctuation \n",
    "punctuations = string.punctuation\n",
    "def clean_punctuation(text):\n",
    "    trs = str.maketrans('', '', punctuations)\n",
    "    return text.translate(trs)\n",
    "## supprimer les nombres \n",
    "def clean_numbers(text):\n",
    "    return re.sub('[0-9]+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cbedb0-39ab-419c-bf69-e427f5ea2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## supprimer les stop words \n",
    "# géneration de la liste des \"mots vide\"(stopwords) avec nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords= stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf839c-9d38-49f0-86fc-b67976e9b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1542b8-b2b7-45f8-905c-18c74472c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS= set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcafb68a-082c-407e-87ae-43f5a6a1304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## supprimer les stopwords \n",
    "def clean_stopword(text):\n",
    "    s = \"\"\n",
    "    for word in str(text).split():\n",
    "        if word not in STOPWORDS:\n",
    "             s+=word+\" \"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad512ed-6a50-4de2-b1d1-c179a4240107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on supprime les mots contenant une seule lettre\n",
    "def clean_shortwords(text):\n",
    "    s=\"\"\n",
    "    for word in str(text).split():\n",
    "        if len(word) > 1:\n",
    "            s+=word+\" \"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20f642-b114-4133-9e68-232f86635b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tweet'] = dataset['tweet'].apply(lambda text: deEmojify(text))\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda text: clean_hash(text))\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda text: remove_mentions(text))\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda text: clean_url(text))\n",
    "dataset['tweet'] = dataset['tweet'].str.lower()\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda text: clean_stopword(text))\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda text: clean_punctuation(text))\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda text: clean_numbers(text))\n",
    "dataset['tweet'] = dataset['tweet'].apply(lambda text: clean_shortwords(text))\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad61e41-73d3-4a45-a3d6-082e0938aec9",
   "metadata": {},
   "source": [
    "### Tokenisation des tweets avec NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7a204-e1a3-45f3-97c3-7a1df443b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b863f09-144d-4f80-a3ea-e1d28c239516",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tweet_tokenized'] = dataset['tweet'].apply(nltk.word_tokenize)\n",
    "dataset['tweet_tokenized'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69d0952-9301-4d37-84fc-af6574442957",
   "metadata": {},
   "source": [
    "### Stemming avec nltk\n",
    " processus de réduction d'un mot à un ou plusieurs racines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa81ba-6745-4f45-942b-05baf00c2fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def stemming(token):\n",
    "    l=[]\n",
    "    for e in token:\n",
    "        l.append(ps.stem(e))\n",
    "    return l\n",
    "dataset['tweet_tokenized']= dataset['tweet_tokenized'].apply(lambda t: stemming(t))\n",
    "dataset['tweet_tokenized'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74432705-fbdd-4594-b870-bb078ea1cb5c",
   "metadata": {},
   "source": [
    "### Lemmatization avec NLTK\n",
    "processus consistant à regrouper les différentes formes infléchies d'un mot afin qu'elles puissent être analysées comme un seul élément"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65864022-5b25-4a25-8f6b-7cf1fa024f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(token):\n",
    "    l=[]\n",
    "    for e in token:\n",
    "        l.append(lm.lemmatize(e))\n",
    "    return l\n",
    "dataset['tweet_tokenized'] = dataset['tweet_tokenized'].apply(lambda t: lemmatizing(t))\n",
    "dataset['tweet_tokenized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf63284-5e65-406e-a4e7-b32002dd65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset['tweet']= [' '.join(map(str, l)) for l in dataset['tweet_tokenized']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22101ce1-b8c7-4498-9518-70d1ca71fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5916a98-30ff-4dbd-b0af-d4dae408dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "storedDf= dataset[['tweet','label']]\n",
    "storedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b868731-d02e-4e1d-a3ec-7f8c167c0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "storedDf.to_csv(\"dataCleaned.csv\",index=False,header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b15404-6f55-4eb0-890b-9379064806a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(dataset[:3360].tweet))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c40f4-f5a4-461d-ab7c-309a388016f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(dataset[3360:].tweet))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99c0ff-4df0-403b-81d4-7db19aa23658",
   "metadata": {},
   "source": [
    "## Méthode Bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4ecc7-ddfb-48ed-b465-1e8a860e636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rom gensim import corpora, models, similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c789a-e0f2-4cd6-af64-f87cd8d66336",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=pd.DataFrame()\n",
    "target['tweet'] = dataset['tweet_tokenized'].apply(lambda x: \" \".join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec210b8-ae2a-4a32-9176-d896d227fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on supprimme les mots dont l'occurence est inférieur à 2\n",
    "coun_vect = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09246d7b-8069-4bef-97d7-b285f7489723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# on vectorise tout les tweets de notre dataset par la methode bag of words \n",
    "count_matrix = coun_vect.fit_transform(target.tweet)\n",
    "count_array = count_matrix.toarray()\n",
    "# on enregistre les données dans une matrice puis sous format csv \n",
    "matrice = pd.DataFrame(data=count_array,columns = coun_vect.get_feature_names())\n",
    "matrice.to_csv(\"matrice.csv\",index=True,header=True)\n",
    "matrice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b6dd1-8b5f-4ab1-932b-5a9c0222b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a823c-5e11-4d16-8f92-684862f646d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow_vectors=matrice.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abef0ed-1cd8-4542-a195-a682506bbdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on ajoute les vecteurs des tweets au dataset\n",
    "\n",
    "#dataset['bow_vectors']=bow_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4463dc0-9bff-416c-8c64-56d3bc40e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.drop(columns=['bow_vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5caf46-647c-446f-a1b6-1bbff4527abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16268f83-f918-4e52-a804-5498add0d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# la longueur de notre vocabulaire\n",
    "vocab_size=len(coun_vect.vocabulary_)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b9193a-7a28-457d-b050-d48048fce4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# on enregistre le vocabulaire dans une lite\n",
    "vocab = list(coun_vect.vocabulary_.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfaab1f-5142-4907-b5bc-d270602c86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizetweet(text,vocab):\n",
    "    d = dict()\n",
    "    for word in vocab:\n",
    "        d[word]=0\n",
    "    for word in str(text).split():\n",
    "        if word  in vocab:\n",
    "            d[word]+=1\n",
    "    return list(d.values())\n",
    "dataset['bow_vectors']= dataset['tweet'].apply(lambda t: vectorizetweet(t,vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89140b-09e8-41d4-8327-000d32c81d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"datasetfinal.csv\",index=True,header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670bd30-c2e3-411a-b1e0-69ba7c711758",
   "metadata": {},
   "source": [
    "## Division du dataset en données d'apprentissage, de test et de validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e03e5a-0775-4cd5-b9fe-ea55f6ea5da0",
   "metadata": {},
   "source": [
    " les données d'apprentissage représentent 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d5685-f125-4f7c-8524-1261bf1aa176",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain, data_remaining, labelsTrain, label_remaining = train_test_split(dataset.bow_vectors,dataset.label,train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798010a5-e590-477c-a3de-c397e9fa1b87",
   "metadata": {},
   "source": [
    "on divise les données restantes en données test et données de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52207485-937b-416f-80f3-4e248ec5f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataValid, dataTest, labelsValid, labelsTest = train_test_split(data_remaining, \n",
    "                                                                         label_remaining , \n",
    "                                                                         test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab1b2c-dd26-454c-be2e-2ce8374f538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataTrain.shape,labelsTrain.shape)\n",
    "print(dataValid.shape,labelsValid.shape)\n",
    "print(dataTest.shape,labelsTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e0f84-9fcb-44e3-bc80-78cc5965e443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efef8e69-283e-4a1d-b5bc-006c660376cb",
   "metadata": {},
   "source": [
    "## Réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8010027-8fc5-4022-8e3d-e7d36ca6acfc",
   "metadata": {},
   "source": [
    "### Création de Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978cf4e-aca7-42c6-8789-6964cb0f331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable utiles \n",
    "BATCH_SIZE=32  #taille du batch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #definition de la machine\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b166c3-0874-4f70-bd38-164d432da993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une classe qui hérite de Dataset et redéfinit les méthodes comme susmentionné\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data,label):\n",
    "        self.bow = data.tolist()\n",
    "        self.targets = label.tolist()\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            self.bow[i],\n",
    "            self.targets[i]\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce046ed-6238-4781-aaf6-4d865bfdc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciation de 3 objets pour les données et labels de l'apprentissage, test et validation\n",
    "train = MyDataset(dataTrain,labelsTrain)\n",
    "test = MyDataset(dataTest,labelsTest)\n",
    "valid = MyDataset(dataValid,labelsValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2b048-c753-4878-98ff-df485d5b2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#réer les objets DataLoader pour les datasets d'apprentissage, test et validation en lui donner la taille du batch convenue\n",
    "# la fonction collate permet de parcourir la listes des vecteurs\n",
    "def collate(batch):\n",
    "    bow = [item[0] for item in batch]\n",
    "    target = torch.LongTensor([item[1] for item in batch])\n",
    "    return  bow, target\n",
    "\n",
    "trainDL = DataLoader(train,batch_size=BATCH_SIZE,shuffle = True,collate_fn=collate)\n",
    "testDL = DataLoader(test,batch_size=BATCH_SIZE,shuffle = False,collate_fn=collate)\n",
    "validDL = DataLoader(valid,batch_size=BATCH_SIZE,shuffle = False,collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e9ddb-5d85-4e0d-8ff5-e7465bf621b4",
   "metadata": {},
   "source": [
    "### Création du réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b93bba-42cc-44f4-986a-ce6980976a39",
   "metadata": {},
   "source": [
    "- Input Layer : le vecteur bag of words\n",
    "- Fonction d'activation  : ReLu\n",
    "- Hidden layer  : applique une transformation linéaire sur les données en input, et réduit leur dimension en 50\n",
    "- Fonction d'activation: Sigmoid\n",
    "- Output Layer: applique une transformation linéaire sur les données en input, et réduit leur dimension en 2 \n",
    "              2 résultats possibles : positif ou négatif\n",
    "              probabilité que le document d'entrée soit classé comme l'étiquette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3e83b-dd25-4689-9c66-f1c54e8890db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taille des hidden layers \n",
    "\n",
    "HIDDEN1 = 50\n",
    "#HIDDEN2 = 100\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608afe5c-15a7-4648-9bf3-df43c2be07a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Création d'une classe qui hérite de nn.Module et redéfinit le constructeur et la méthode forward\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, device, vocab_size, hidden1, num_labels, batch_size):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden1)\n",
    "      #  self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden1, num_labels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = len(x)\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "        x = torch.FloatTensor(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "      #  x = F.relu(self.fc2(x))\n",
    "\n",
    "        return torch.sigmoid(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2497d-c044-431a-930e-67e05296ecac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instanciation de notre modele \n",
    "my_model = MyNetwork(\n",
    "    vocab_size= vocab_size,\n",
    "    hidden1=HIDDEN1,\n",
    "   #     hidden2=HIDDEN2,\n",
    "\n",
    "    num_labels=2,\n",
    "    device=device,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7310700-9f86-431a-a4c0-e08b73d864e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Entraîner le réseau de neuronne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f0159-4413-4d2b-95ae-647d6ae6f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAUX D'APPRENTISSAGE\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080966c5-670a-4763-90b0-f34c3df68281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  la fonction du coût choisit est :  CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# la fonction d'optimisation choisit est : Adam\n",
    "optimizer= optim.Adam(my_model.parameters(),lr=LEARNING_RATE)\n",
    "# fct de changement du learning rate lors de chaque epoch \n",
    "scheduler = CosineAnnealingLR(optimizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3773d1-aea3-4d01-92fe-cd717d5ba01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction d'apprentissage\n",
    "def train_epoch(model, optimizer, train_loader):\n",
    "        model.train()\n",
    "        total_loss, total = 0, 0\n",
    "        for dt in train_loader:\n",
    "            inputs,target = dt\n",
    " # Réinitialiser le gradient\n",
    "            optimizer.zero_grad()\n",
    "# Passe en avant\n",
    "\n",
    "            output = model(torch.Tensor(inputs))\n",
    "# Calculer le coût en comparant les labels prédits aux targets du minibatch\n",
    "            loss = criterion(output, target)\n",
    "# Faire la backpropagation\n",
    "            loss.backward()\n",
    " # Effectuer un pas d'optimisation\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "# Mettre à jour votre coût d'apprentissage en lui ajoutant le coût du data batch\n",
    "            total_loss += loss.item()\n",
    "            total += len(target)\n",
    "#  la sortie est le coût moyen pour toutes les données training\n",
    "        return total_loss / total\n",
    "        \n",
    "    \n",
    "# fonction de validation\n",
    "def validate_epoch(model, valid_loader):\n",
    "    model.eval()\n",
    "    total_loss, total = 0, 0\n",
    "# Spécifier qu'on est sur le mode d'évaluation\n",
    "    with torch.no_grad():\n",
    "        for dt in valid_loader:\n",
    "            inputs,target = dt\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(torch.Tensor(inputs))\n",
    "\n",
    "            # Calculer le coût en comparant les labels prédits aux targets du minibatch\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Mettre à jour votre coût d'apprentissage en lui ajoutant le coût du data batch\n",
    "            total_loss += loss.item()\n",
    "            total += len(target)\n",
    "\n",
    "    return total_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01d7c3-8f05-47a9-99d2-c43b9894cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "n_epochs = 10 # le nombre d'epochs est petit vu la taille du dataset.\n",
    "# boucle sur les epochs:\n",
    "start = time.time()\n",
    "for i in range(n_epochs):\n",
    "    train_loss = train_epoch(my_model, optimizer, trainDL)\n",
    "    valid_loss = validate_epoch(my_model, validDL)\n",
    "    \n",
    "    tqdm.write(\n",
    "        f'epoch #{i + 1:3d}\\ttrain_loss: {train_loss:.2}\\tvalid_loss: {valid_loss:.2}\\n',\n",
    "    )\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18a1c2-dd73-4b88-a4e4-88e81504ef70",
   "metadata": {},
   "source": [
    "### Test du modèle"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1054f647-a167-4dc6-a3a6-69d7e5c0a522",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ccbbac-c0d4-443c-a05d-26f677fc1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilisation du  coût de test à 0\n",
    "\n",
    "test_loss = 0\n",
    "\n",
    "# Initialisation du  nombre de prévisions correctes à 0\n",
    "correct = 0\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for dt in testDL:\n",
    "        inputs,target = dt\n",
    "       # Forward pass\n",
    "        output = my_model(torch.Tensor(inputs))\n",
    "        probs= output\n",
    "       # Calcul du  coût en comparant les labels prédits aux targets du minibatch\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        # On  compare le label prédit avec le labels du minibatch et on met à jour le nbre de prévisions correctes\n",
    "        correct += torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        target = target.cpu().numpy()\n",
    "        y_true.extend(predictions)\n",
    "        y_pred.extend(target)\n",
    "        \n",
    "        \n",
    "\n",
    "    test_loss /= len(testDL)\n",
    "# Calculer la précision: la moyenne des prévisions correctes sur l'ensemble des observations dans le dataset test\n",
    "    correct /= len(testDL.dataset) #10.000  \n",
    "print(f\"Test loss {test_loss*100:.2f}%\")\n",
    "\n",
    "print(f\"Accuracy {correct*100:.2f}%\")\n",
    "print(classification_report(y_true, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f749332-a1fc-4e93-a531-154f62f6d78e",
   "metadata": {},
   "source": [
    "## Déploiement du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da2707-f421-4246-aec4-151baf17f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list()\n",
    "from IPython.core.display import display, HTML\n",
    "def print_random_prediction(Testdataset,model, n=5):\n",
    "    to_value = lambda x: 'True' if x else 'False'\n",
    "    model.eval()\n",
    "    rows = []\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            random_idx = random.randint(0,len(Testdataset)-1)\n",
    "            print(random_idx)\n",
    "            text, target, bow,tweet_tokenized =Testdataset.iloc[int(random_idx)]\n",
    "            \n",
    "            inputs = bow\n",
    "            l=bow\n",
    "            probs = model(torch.Tensor(inputs))\n",
    "            probs = probs.detach().cpu().numpy()\n",
    "            prediction = np.argmax(probs)\n",
    "            print(probs)\n",
    "            predicted = to_value(prediction)\n",
    "            actual = to_value(target)\n",
    "            \n",
    "            row = f\"\"\"\n",
    "            <tr>\n",
    "            <td>{i+1}&nbsp;</td>\n",
    "            <td>{text}&nbsp;</td>\n",
    "            <td>{predicted}&nbsp;</td>\n",
    "            <td>{actual}&nbsp;</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "            rows.append(row)\n",
    "    \n",
    "    rows_joined = '\\n'.join(rows)\n",
    "    table = f\"\"\"\n",
    "    <table>\n",
    "    <tbody>\n",
    "    <tr>\n",
    "    <td><b>Tweet</b>&nbsp;</td>\n",
    "    <td><b>Predicted</b>&nbsp;</td>\n",
    "    <td><b>Actual</b>&nbsp;</td>\n",
    "    </tr>{rows_joined}\n",
    "    </tbody>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    display(HTML(table))\n",
    "Testdataset=dataset[['tweetsave','label','bow_vectors','tweet_tokenized']].sample(100)\n",
    "print_random_prediction(Testdataset,my_model, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2355fe-8ba4-4ff8-ae81-a22727aec3ea",
   "metadata": {},
   "source": [
    "### Exemple de prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e9566-8506-40ef-990d-58c0769a3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## probleme ici \n",
    "def cleantweet(text):\n",
    "    text = deEmojify(text)\n",
    "    text = clean_hash(text)\n",
    "    text = remove_mentions(text)\n",
    "    text =  clean_url(text)\n",
    "    text = text.lower()\n",
    "    text = clean_stopword(text)\n",
    "    text = clean_punctuation(text)\n",
    "    text = clean_numbers(text)\n",
    "    text = stemming(text)\n",
    "    text = lemmatizing(text)\n",
    "    return ''.join(map(str, text)) \n",
    "\n",
    "\n",
    "def vectorizetweet(text,vocab):\n",
    "    d = dict()\n",
    "    for word in vocab:\n",
    "        d[word]=0\n",
    "    for word in str(text).split():\n",
    "        if word  in vocab:\n",
    "            d[word]+=1\n",
    "    return list(d.values())\n",
    "\n",
    "\n",
    "def makeprediction(tweet_vector,model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            probs = model(torch.Tensor(tweet_vector))\n",
    "            probs = probs.detach().cpu().numpy()\n",
    "            prediction = np.argmax(probs)\n",
    "            print(probs)\n",
    "            print(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b0534-3d51-44bb-8b21-7d1ff405cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"RT @HHSGov: Looking for information about #COVID19 testing? Find the latest – including how to get a test in your community – on our update… \t\"\n",
    "tweet = cleantweet(text)\n",
    "l2=vectorizetweet(tweet,vocab)\n",
    "if makeprediction(l2,my_model)==0:\n",
    "    print(\"false\")\n",
    "else: print(\"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69555c4e-8fff-4f65-89bf-d6903d438ea4",
   "metadata": {},
   "source": [
    "### Enregistrement du model avec pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fb3c2-36dd-436e-b8e8-979b1cf93598",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(my_model, \"fakenews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283008e-a3f6-4ae8-86a7-c3465e30d0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94590c14-dffa-4cbf-8d01-2b1d1243ef74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
